{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9ee671",
   "metadata": {},
   "source": [
    "# Breast Cancer EDA & Modeling Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52680588",
   "metadata": {},
   "source": [
    "## Imports & Global Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e5b4f",
   "metadata": {},
   "source": [
    "## Quick Guide: Running Manual vs Agentic EDA\n",
    "\n",
    "This notebook contains both a manual EDA workflow and an automated `EDAAgent` for a friendly comparison. Follow these steps to reproduce the demo:\n",
    "\n",
    "1. Install dependencies: `pip install -r requirements.txt` (see repository root).\n",
    "2. Open this notebook in Jupyter and run cells top-to-bottom to ensure all variables are initialized.\n",
    "3. Manual EDA: run the cells up through the 'Manual EDA Summary' section for the human-driven exploration and visualizations.\n",
    "4. Agentic EDA: run the cell in the 'Running the EDA Agent' section to execute `EDAAgent` — watch the timestamped logs it prints.\n",
    "5. Agentic FE demo: a short, non-redundant 'Agentic-style Feature Engineering Demo' appears just before 'Baseline Feature Engineering' — it reuses existing helper functions to demonstrate reproducible FE in one compact step.\n",
    "\n",
    "For professor review: show the Agent Execution Log and the Comparison section (Manual vs. Agentic) — they demonstrate reproducibility, auditability, and parity with the manual workflow.\n",
    "\n",
    "If you need a clean environment, use the included `requirements.txt` and a venv. See the project root for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d313fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import importlib.util\n",
    "import inspect\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Display + plotting style\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f87cbc",
   "metadata": {},
   "source": [
    "## Repository Root & Project Paths\n",
    "This cell detects the project’s root folder and sets up all directory paths (data, engineered data, artifacts) so the notebook can reliably read and save files no matter where it’s run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8b96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\rajni\\Documents\\breast-cancer-agentic\n",
      "Using source file: C:\\Users\\rajni\\Documents\\breast-cancer-agentic\\data\\raw\\breast_cancer_with_columns.csv\n"
     ]
    }
   ],
   "source": [
    "# Find the project root so all file paths work no matter where the notebook is run\n",
    "def get_repo_root():\n",
    "    try:\n",
    "        # Ask Git for the top-level directory of this repository\n",
    "        return Path(subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], text=True).strip())\n",
    "    except Exception:\n",
    "        # Walk up a few levels looking for a .git folder\n",
    "        p = Path.cwd()\n",
    "        for _ in range(6):\n",
    "            if (p / \".git\").exists():\n",
    "                return p\n",
    "            p = p.parent\n",
    "        # Fallback: current working directory\n",
    "        return Path.cwd()\n",
    "\n",
    "try:\n",
    "    ROOT = get_repo_root()\n",
    "except Exception:\n",
    "    ROOT = Path.cwd()\n",
    "\n",
    "FALLBACK_ROOT = Path(r\"C:\\Users\\rajni\\Documents\\breast-cancer-agentic\")\n",
    "if ROOT != FALLBACK_ROOT and not (ROOT / \"data\" / \"raw\").exists():\n",
    "    ROOT = FALLBACK_ROOT\n",
    "\n",
    "print(\"Repo root:\", ROOT)\n",
    "\n",
    "# Define key directories\n",
    "DATA_RAW        = ROOT / \"data\" / \"raw\"\n",
    "DATA_ENGINEERED = ROOT / \"data\" / \"engineered\"\n",
    "ARTIFACTS_ENG   = ROOT / \"artifacts\" / \"engineering\"\n",
    "ARTIFACTS_EDA   = ROOT / \"artifacts\" / \"eda\"\n",
    "\n",
    "for p in [DATA_ENGINEERED, ARTIFACTS_ENG, ARTIFACTS_EDA]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Source CSV + target column\n",
    "SRC_FILE   = DATA_RAW / \"breast_cancer_with_columns.csv\"\n",
    "TARGET_COL = \"diagnosis\"\n",
    "\n",
    "print(\"Using source file:\", SRC_FILE)\n",
    "assert SRC_FILE.exists(), f\"Missing source file: {SRC_FILE}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0023fb",
   "metadata": {},
   "source": [
    "## Load Dataset & Basic Checks\n",
    "This cell loads the raw dataset, verifies that the target column exists, splits the data into features and labels, and provides basic previews (head, summary stats, missing values) to confirm the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ec8c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (569, 32)\n",
      "Target distribution (normalized):\n",
      "diagnosis\n",
      "B    0.627417\n",
      "M    0.372583\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  fractal_dimension_mean  radius_se  texture_se  perimeter_se  \\\n",
       "0         0.2419                 0.07871     1.0950      0.9053         8.589   \n",
       "1         0.1812                 0.05667     0.5435      0.7339         3.398   \n",
       "2         0.2069                 0.05999     0.7456      0.7869         4.585   \n",
       "3         0.2597                 0.09744     0.4956      1.1560         3.445   \n",
       "4         0.1809                 0.05883     0.7572      0.7813         5.438   \n",
       "\n",
       "   area_se  smoothness_se  compactness_se  concavity_se  concave points_se  \\\n",
       "0   153.40       0.006399         0.04904       0.05373            0.01587   \n",
       "1    74.08       0.005225         0.01308       0.01860            0.01340   \n",
       "2    94.03       0.006150         0.04006       0.03832            0.02058   \n",
       "3    27.23       0.009110         0.07458       0.05661            0.01867   \n",
       "4    94.44       0.011490         0.02461       0.05688            0.01885   \n",
       "\n",
       "   symmetry_se  fractal_dimension_se  radius_worst  texture_worst  \\\n",
       "0      0.03003              0.006193         25.38          17.33   \n",
       "1      0.01389              0.003532         24.99          23.41   \n",
       "2      0.02250              0.004571         23.57          25.53   \n",
       "3      0.05963              0.009208         14.91          26.50   \n",
       "4      0.01756              0.005115         22.54          16.67   \n",
       "\n",
       "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0           184.60      2019.0            0.1622             0.6656   \n",
       "1           158.80      1956.0            0.1238             0.1866   \n",
       "2           152.50      1709.0            0.1444             0.4245   \n",
       "3            98.87       567.7            0.2098             0.8663   \n",
       "4           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0           0.7119                0.2654          0.4601   \n",
       "1           0.2416                0.1860          0.2750   \n",
       "2           0.4504                0.2430          0.3613   \n",
       "3           0.6869                0.2575          0.6638   \n",
       "4           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30371831.432337</td>\n",
       "      <td>125020585.612224</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>869218.0</td>\n",
       "      <td>906024.0</td>\n",
       "      <td>8813129.0</td>\n",
       "      <td>911320502.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosis</th>\n",
       "      <td>569</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981</td>\n",
       "      <td>11.7</td>\n",
       "      <td>13.37</td>\n",
       "      <td>15.78</td>\n",
       "      <td>28.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.71</td>\n",
       "      <td>16.17</td>\n",
       "      <td>18.84</td>\n",
       "      <td>21.8</td>\n",
       "      <td>39.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.79</td>\n",
       "      <td>75.17</td>\n",
       "      <td>86.24</td>\n",
       "      <td>104.1</td>\n",
       "      <td>188.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.5</td>\n",
       "      <td>420.3</td>\n",
       "      <td>551.1</td>\n",
       "      <td>782.7</td>\n",
       "      <td>2501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09636</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.08637</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.09263</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.3454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.07972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.4268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02031</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.00706</td>\n",
       "      <td>0.04996</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>0.06612</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.3242</td>\n",
       "      <td>0.4789</td>\n",
       "      <td>2.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.3602</td>\n",
       "      <td>0.8339</td>\n",
       "      <td>1.108</td>\n",
       "      <td>1.474</td>\n",
       "      <td>4.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757</td>\n",
       "      <td>1.606</td>\n",
       "      <td>2.287</td>\n",
       "      <td>3.357</td>\n",
       "      <td>21.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802</td>\n",
       "      <td>17.85</td>\n",
       "      <td>24.53</td>\n",
       "      <td>45.19</td>\n",
       "      <td>542.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.00638</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.02045</td>\n",
       "      <td>0.03245</td>\n",
       "      <td>0.1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01509</td>\n",
       "      <td>0.02589</td>\n",
       "      <td>0.04205</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.00617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.01093</td>\n",
       "      <td>0.01471</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.01516</td>\n",
       "      <td>0.01873</td>\n",
       "      <td>0.02348</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.26919</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.93</td>\n",
       "      <td>13.01</td>\n",
       "      <td>14.97</td>\n",
       "      <td>18.79</td>\n",
       "      <td>36.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.02</td>\n",
       "      <td>21.08</td>\n",
       "      <td>25.41</td>\n",
       "      <td>29.72</td>\n",
       "      <td>49.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.41</td>\n",
       "      <td>84.11</td>\n",
       "      <td>97.66</td>\n",
       "      <td>125.4</td>\n",
       "      <td>251.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.2</td>\n",
       "      <td>515.3</td>\n",
       "      <td>686.5</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>4254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.2226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>1.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.3829</td>\n",
       "      <td>1.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.09993</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.6638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.05504</td>\n",
       "      <td>0.07146</td>\n",
       "      <td>0.08004</td>\n",
       "      <td>0.09208</td>\n",
       "      <td>0.2075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count unique  top freq             mean  \\\n",
       "id                       569.0    NaN  NaN  NaN  30371831.432337   \n",
       "diagnosis                  569      2    B  357              NaN   \n",
       "radius_mean              569.0    NaN  NaN  NaN        14.127292   \n",
       "texture_mean             569.0    NaN  NaN  NaN        19.289649   \n",
       "perimeter_mean           569.0    NaN  NaN  NaN        91.969033   \n",
       "area_mean                569.0    NaN  NaN  NaN       654.889104   \n",
       "smoothness_mean          569.0    NaN  NaN  NaN          0.09636   \n",
       "compactness_mean         569.0    NaN  NaN  NaN         0.104341   \n",
       "concavity_mean           569.0    NaN  NaN  NaN         0.088799   \n",
       "concave points_mean      569.0    NaN  NaN  NaN         0.048919   \n",
       "symmetry_mean            569.0    NaN  NaN  NaN         0.181162   \n",
       "fractal_dimension_mean   569.0    NaN  NaN  NaN         0.062798   \n",
       "radius_se                569.0    NaN  NaN  NaN         0.405172   \n",
       "texture_se               569.0    NaN  NaN  NaN         1.216853   \n",
       "perimeter_se             569.0    NaN  NaN  NaN         2.866059   \n",
       "area_se                  569.0    NaN  NaN  NaN        40.337079   \n",
       "smoothness_se            569.0    NaN  NaN  NaN         0.007041   \n",
       "compactness_se           569.0    NaN  NaN  NaN         0.025478   \n",
       "concavity_se             569.0    NaN  NaN  NaN         0.031894   \n",
       "concave points_se        569.0    NaN  NaN  NaN         0.011796   \n",
       "symmetry_se              569.0    NaN  NaN  NaN         0.020542   \n",
       "fractal_dimension_se     569.0    NaN  NaN  NaN         0.003795   \n",
       "radius_worst             569.0    NaN  NaN  NaN         16.26919   \n",
       "texture_worst            569.0    NaN  NaN  NaN        25.677223   \n",
       "perimeter_worst          569.0    NaN  NaN  NaN       107.261213   \n",
       "area_worst               569.0    NaN  NaN  NaN       880.583128   \n",
       "smoothness_worst         569.0    NaN  NaN  NaN         0.132369   \n",
       "compactness_worst        569.0    NaN  NaN  NaN         0.254265   \n",
       "concavity_worst          569.0    NaN  NaN  NaN         0.272188   \n",
       "concave points_worst     569.0    NaN  NaN  NaN         0.114606   \n",
       "symmetry_worst           569.0    NaN  NaN  NaN         0.290076   \n",
       "fractal_dimension_worst  569.0    NaN  NaN  NaN         0.083946   \n",
       "\n",
       "                                      std       min       25%       50%  \\\n",
       "id                       125020585.612224    8670.0  869218.0  906024.0   \n",
       "diagnosis                             NaN       NaN       NaN       NaN   \n",
       "radius_mean                      3.524049     6.981      11.7     13.37   \n",
       "texture_mean                     4.301036      9.71     16.17     18.84   \n",
       "perimeter_mean                  24.298981     43.79     75.17     86.24   \n",
       "area_mean                      351.914129     143.5     420.3     551.1   \n",
       "smoothness_mean                  0.014064   0.05263   0.08637   0.09587   \n",
       "compactness_mean                 0.052813   0.01938   0.06492   0.09263   \n",
       "concavity_mean                    0.07972       0.0   0.02956   0.06154   \n",
       "concave points_mean              0.038803       0.0   0.02031    0.0335   \n",
       "symmetry_mean                    0.027414     0.106    0.1619    0.1792   \n",
       "fractal_dimension_mean            0.00706   0.04996    0.0577   0.06154   \n",
       "radius_se                        0.277313    0.1115    0.2324    0.3242   \n",
       "texture_se                       0.551648    0.3602    0.8339     1.108   \n",
       "perimeter_se                     2.021855     0.757     1.606     2.287   \n",
       "area_se                         45.491006     6.802     17.85     24.53   \n",
       "smoothness_se                    0.003003  0.001713  0.005169   0.00638   \n",
       "compactness_se                   0.017908  0.002252   0.01308   0.02045   \n",
       "concavity_se                     0.030186       0.0   0.01509   0.02589   \n",
       "concave points_se                 0.00617       0.0  0.007638   0.01093   \n",
       "symmetry_se                      0.008266  0.007882   0.01516   0.01873   \n",
       "fractal_dimension_se             0.002646  0.000895  0.002248  0.003187   \n",
       "radius_worst                     4.833242      7.93     13.01     14.97   \n",
       "texture_worst                    6.146258     12.02     21.08     25.41   \n",
       "perimeter_worst                 33.602542     50.41     84.11     97.66   \n",
       "area_worst                     569.356993     185.2     515.3     686.5   \n",
       "smoothness_worst                 0.022832   0.07117    0.1166    0.1313   \n",
       "compactness_worst                0.157336   0.02729    0.1472    0.2119   \n",
       "concavity_worst                  0.208624       0.0    0.1145    0.2267   \n",
       "concave points_worst             0.065732       0.0   0.06493   0.09993   \n",
       "symmetry_worst                   0.061867    0.1565    0.2504    0.2822   \n",
       "fractal_dimension_worst          0.018061   0.05504   0.07146   0.08004   \n",
       "\n",
       "                               75%          max  \n",
       "id                       8813129.0  911320502.0  \n",
       "diagnosis                      NaN          NaN  \n",
       "radius_mean                  15.78        28.11  \n",
       "texture_mean                  21.8        39.28  \n",
       "perimeter_mean               104.1        188.5  \n",
       "area_mean                    782.7       2501.0  \n",
       "smoothness_mean             0.1053       0.1634  \n",
       "compactness_mean            0.1304       0.3454  \n",
       "concavity_mean              0.1307       0.4268  \n",
       "concave points_mean          0.074       0.2012  \n",
       "symmetry_mean               0.1957        0.304  \n",
       "fractal_dimension_mean     0.06612      0.09744  \n",
       "radius_se                   0.4789        2.873  \n",
       "texture_se                   1.474        4.885  \n",
       "perimeter_se                 3.357        21.98  \n",
       "area_se                      45.19        542.2  \n",
       "smoothness_se             0.008146      0.03113  \n",
       "compactness_se             0.03245       0.1354  \n",
       "concavity_se               0.04205        0.396  \n",
       "concave points_se          0.01471      0.05279  \n",
       "symmetry_se                0.02348      0.07895  \n",
       "fractal_dimension_se      0.004558      0.02984  \n",
       "radius_worst                 18.79        36.04  \n",
       "texture_worst                29.72        49.54  \n",
       "perimeter_worst              125.4        251.2  \n",
       "area_worst                  1084.0       4254.0  \n",
       "smoothness_worst             0.146       0.2226  \n",
       "compactness_worst           0.3391        1.058  \n",
       "concavity_worst             0.3829        1.252  \n",
       "concave points_worst        0.1614        0.291  \n",
       "symmetry_worst              0.3179       0.6638  \n",
       "fractal_dimension_worst    0.09208       0.2075  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values (top 15):\n",
      "id                        0\n",
      "diagnosis                 0\n",
      "radius_mean               0\n",
      "texture_mean              0\n",
      "perimeter_mean            0\n",
      "area_mean                 0\n",
      "smoothness_mean           0\n",
      "compactness_mean          0\n",
      "concavity_mean            0\n",
      "concave points_mean       0\n",
      "symmetry_mean             0\n",
      "fractal_dimension_mean    0\n",
      "radius_se                 0\n",
      "texture_se                0\n",
      "perimeter_se              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset from disk\n",
    "df = pd.read_csv(SRC_FILE)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n",
    "# Make sure the target column exists before continuing\n",
    "if TARGET_COL not in df.columns:\n",
    "    print(\"Columns available (first 40):\", list(df.columns)[:40])\n",
    "    raise AssertionError(f\"Target column '{TARGET_COL}' not found in CSV\")\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "print(\"Target distribution (normalized):\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Glance at the data and summary statistics to sanity-check values and types\n",
    "display(df.head())\n",
    "display(df.describe(include=\"all\").T)\n",
    "\n",
    "print(\"Missing values (top 15):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a10ef8",
   "metadata": {},
   "source": [
    "## Data Quality Report\n",
    "This cell provides a comprehensive summary of data quality: missing value rates, duplicate rows, data type distribution, and numeric ranges. These metrics help identify potential data issues before modeling and inform cleaning strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c18877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY REPORT\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: 569 rows × 32 columns\n",
      "Duplicate Rows: 0 (0.00%)\n",
      "\n",
      " No missing values detected.\n",
      "\n",
      "Data Type Distribution:\n",
      "  float64: 30 columns\n",
      "  int64: 1 columns\n",
      "  object: 1 columns\n",
      "\n",
      "Numeric Column Ranges (min, max, mean):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>8670.00000</td>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>1.250206e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>radius_mean</td>\n",
       "      <td>6.98100</td>\n",
       "      <td>2.811000e+01</td>\n",
       "      <td>1.412729e+01</td>\n",
       "      <td>3.524049e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texture_mean</td>\n",
       "      <td>9.71000</td>\n",
       "      <td>3.928000e+01</td>\n",
       "      <td>1.928965e+01</td>\n",
       "      <td>4.301036e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perimeter_mean</td>\n",
       "      <td>43.79000</td>\n",
       "      <td>1.885000e+02</td>\n",
       "      <td>9.196903e+01</td>\n",
       "      <td>2.429898e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area_mean</td>\n",
       "      <td>143.50000</td>\n",
       "      <td>2.501000e+03</td>\n",
       "      <td>6.548891e+02</td>\n",
       "      <td>3.519141e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smoothness_mean</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>1.634000e-01</td>\n",
       "      <td>9.636028e-02</td>\n",
       "      <td>1.406413e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>compactness_mean</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>3.454000e-01</td>\n",
       "      <td>1.043410e-01</td>\n",
       "      <td>5.281276e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>concavity_mean</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.268000e-01</td>\n",
       "      <td>8.879932e-02</td>\n",
       "      <td>7.971981e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>concave points_mean</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.012000e-01</td>\n",
       "      <td>4.891915e-02</td>\n",
       "      <td>3.880284e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>symmetry_mean</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>3.040000e-01</td>\n",
       "      <td>1.811619e-01</td>\n",
       "      <td>2.741428e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                column         min           max          mean           std\n",
       "0                   id  8670.00000  9.113205e+08  3.037183e+07  1.250206e+08\n",
       "1          radius_mean     6.98100  2.811000e+01  1.412729e+01  3.524049e+00\n",
       "2         texture_mean     9.71000  3.928000e+01  1.928965e+01  4.301036e+00\n",
       "3       perimeter_mean    43.79000  1.885000e+02  9.196903e+01  2.429898e+01\n",
       "4            area_mean   143.50000  2.501000e+03  6.548891e+02  3.519141e+02\n",
       "5      smoothness_mean     0.05263  1.634000e-01  9.636028e-02  1.406413e-02\n",
       "6     compactness_mean     0.01938  3.454000e-01  1.043410e-01  5.281276e-02\n",
       "7       concavity_mean     0.00000  4.268000e-01  8.879932e-02  7.971981e-02\n",
       "8  concave points_mean     0.00000  2.012000e-01  4.891915e-02  3.880284e-02\n",
       "9        symmetry_mean     0.10600  3.040000e-01  1.811619e-01  2.741428e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Data quality checks complete. Ready for EDA and modeling.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Missing values summary and percentage\n",
    "missing_summary = pd.DataFrame({\n",
    "    'column': df.columns,\n",
    "    'n_missing': [df[c].isna().sum() for c in df.columns],\n",
    "    'pct_missing': [100 * df[c].isna().sum() / len(df) for c in df.columns],\n",
    "    'dtype': df.dtypes.values,\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['n_missing'] > 0].sort_values('n_missing', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Check for duplicates\n",
    "n_duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate Rows: {n_duplicates} ({100 * n_duplicates / len(df):.2f}%)\")\n",
    "\n",
    "# Missing values\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"\\nMissing Values (columns with missing data):\")\n",
    "    display(missing_summary[['column', 'n_missing', 'pct_missing', 'dtype']])\n",
    "else:\n",
    "    print(f\"\\n No missing values detected.\")\n",
    "\n",
    "# Data type distribution\n",
    "print(f\"\\nData Type Distribution:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Numeric column ranges\n",
    "print(f\"\\nNumeric Column Ranges (min, max, mean):\")\n",
    "numeric_ranges = []\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    numeric_ranges.append({\n",
    "        'column': col,\n",
    "        'min': df[col].min(),\n",
    "        'max': df[col].max(),\n",
    "        'mean': df[col].mean(),\n",
    "        'std': df[col].std(),\n",
    "    })\n",
    "numeric_df = pd.DataFrame(numeric_ranges)\n",
    "display(numeric_df.head(10))\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"Data quality checks complete. Ready for EDA and modeling.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d04e1",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing\n",
    "This cell performs t-tests and chi-square tests to quantify whether observed feature differences between classes are statistically significant (beyond random chance). Results inform feature selection and help flag spurious correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e03970-727d-4d50-a4c4-5533f378556d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from statsmodels) (2.1.2)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from statsmodels) (1.16.3)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rajni\\documents\\breast-cancer-agentic\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6cc6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Perform t-tests to assess feature significance vs. target\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Separate feature values by class\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m class_0 = \u001b[43my_bin\u001b[49m.unique()[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m class_1 = y_bin.unique()[\u001b[32m1\u001b[39m]\n\u001b[32m      8\u001b[39m X_class_0 = X[y_bin == class_0]\n",
      "\u001b[31mNameError\u001b[39m: name 'y_bin' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Perform t-tests to assess feature significance vs. target\n",
    "# Separate feature values by class\n",
    "class_0 = y_bin.unique()[0]\n",
    "class_1 = y_bin.unique()[1]\n",
    "\n",
    "X_class_0 = X[y_bin == class_0]\n",
    "X_class_1 = X[y_bin == class_1]\n",
    "\n",
    "# Compute t-stats and p-values for each numeric feature\n",
    "test_results = []\n",
    "for col in X.select_dtypes(include=[np.number]).columns:\n",
    "    t_stat, p_val = stats.ttest_ind(X_class_0[col].dropna(), X_class_1[col].dropna())\n",
    "    test_results.append({\n",
    "        'feature': col,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_val,\n",
    "        'significant': 'Yes' if p_val < 0.05 else 'No',\n",
    "        'mean_class_0': X_class_0[col].mean(),\n",
    "        'mean_class_1': X_class_1[col].mean(),\n",
    "    })\n",
    "\n",
    "significance_df = pd.DataFrame(test_results).sort_values('p_value')\n",
    "print(\"Features ranked by statistical significance (t-test, α=0.05):\")\n",
    "display(significance_df.head(15))\n",
    "\n",
    "# Log summary\n",
    "n_significant = (significance_df['p_value'] < 0.05).sum()\n",
    "print(f\"\\nTotal numeric features: {len(X.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"Statistically significant features (p < 0.05): {n_significant}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335f718",
   "metadata": {},
   "source": [
    "## Numeric Overview & KDE Plots\n",
    "This cell reviews all numeric columns, checks missing/unique counts, and generates KDE plots to visualize how feature distributions differ between malignant and benign cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numeric overview and sample KDE plots\n",
    "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "print(\"Numeric columns count:\", len(num_cols))\n",
    "display(pd.DataFrame({\n",
    "    \"col\": num_cols,\n",
    "    \"n_missing\": [X[c].isna().sum() for c in num_cols],\n",
    "    \"n_unique\": [X[c].nunique() for c in num_cols],\n",
    "}).sort_values([\"n_missing\", \"n_unique\"], ascending=[False, True]).head(20))\n",
    "\n",
    "# Plot a few features\n",
    "sel = num_cols[:6]\n",
    "for col in sel:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.kdeplot(data=df, x=col, hue=TARGET_COL, fill=True, common_norm=False)\n",
    "    plt.title(f\"{col} by {TARGET_COL}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450a364-54d2-4554-b85c-228b71a8d151",
   "metadata": {},
   "source": [
    "## Correlation Heatmap\n",
    "This heatmap shows pairwise correlations between all numeric features.\n",
    "It helps identify groups of highly related features and potential redundancy\n",
    "before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc0e01-6c54-48c4-81f9-fc43999373cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap for All 30 Features\n",
    "plt.figure(figsize=(10,8))\n",
    "# Compute correlation matrix\n",
    "corr = X.corr()\n",
    "# Heatmap visualization\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc2901",
   "metadata": {},
   "source": [
    "## Outlier Detection & Visualization\n",
    "This cell identifies univariate outliers using the IQR method (values beyond 1.5×IQR) and flags extreme values. This helps understand what gets clipped during percentile capping and validates the choice of clip thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIER SUMMARY USING IQR\n",
    "\n",
    "# Create a list to store outlier information for each feature\n",
    "outlier_summary = []\n",
    "\n",
    "# Loop through all numeric columns in X\n",
    "for col in X.select_dtypes(include=[np.number]).columns:\n",
    "    \n",
    "    # Compute the 25th and 75th percentiles\n",
    "    Q1, Q3 = X[col].quantile([0.25, 0.75])\n",
    "    \n",
    "    # Calculate Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define lower and upper bounds for outliers\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify points outside the IQR bounds\n",
    "    outliers = X[(X[col] < lower) | (X[col] > upper)][col]\n",
    "\n",
    "    # Store results for this feature\n",
    "    outlier_summary.append({\n",
    "        \"feature\": col,\n",
    "        \"n_outliers\": len(outliers),                    # Number of outlier values\n",
    "        \"pct_outliers\": round(100 * len(outliers) / len(X), 2),   # Outlier percentage\n",
    "        \"lower_bound\": lower,\n",
    "        \"upper_bound\": upper,\n",
    "        \"min_val\": X[col].min(),\n",
    "        \"max_val\": X[col].max(),\n",
    "    })\n",
    "\n",
    "# Create a summary DataFrame and sort by number of outliers\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values(\"n_outliers\", ascending=False)\n",
    "\n",
    "print(\"Outlier Summary (IQR method):\")\n",
    "display(outlier_df[outlier_df[\"n_outliers\"] > 0].head(10))\n",
    "\n",
    "\n",
    "# BOXPLOTS FOR TOP 6 FEATURES WITH THE MOST OUTLIERS\n",
    "\n",
    "# Select the top 6 features containing the most outliers\n",
    "top_cols = outlier_df[outlier_df[\"n_outliers\"] > 0][\"feature\"].head(6).tolist()\n",
    "\n",
    "if top_cols:\n",
    "    \n",
    "    # Set up a 2x3 grid for plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Create boxplots for each selected feature\n",
    "    for idx, col in enumerate(top_cols):\n",
    "        \n",
    "        # Draw boxplot comparing class 0 and class 1\n",
    "        axes[idx].boxplot(\n",
    "            [X_class_0[col].dropna(), X_class_1[col].dropna()],\n",
    "            labels=[f\"Class {class_0}\", f\"Class {class_1}\"]\n",
    "        )\n",
    "        \n",
    "        # Get outlier count for plot title\n",
    "        count = outlier_df.loc[outlier_df[\"feature\"] == col, \"n_outliers\"].values[0]\n",
    "        \n",
    "        # Add feature name and outlier count\n",
    "        axes[idx].set_title(f\"{col} ({count} outliers)\")\n",
    "        axes[idx].set_ylabel(\"Value\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the boxplots\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nNote: 1st–99th percentile capping can be applied to reduce the impact of these outliers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93238e",
   "metadata": {},
   "source": [
    "## Target Distribution Plot\n",
    "This cell creates a simple countplot of the diagnosis column to show the class distribution (malignant vs. benign) and verify the dataset’s balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83974cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=TARGET_COL, data=df,\n",
    "    palette=[\"#FF6B6B\", \"#4D96FF\"]   # red (malignant), blue (benign)\n",
    ")\n",
    "plt.title(\"Distribution of Diagnosis\")\n",
    "plt.xlabel(\"Diagnosis\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4315b8f",
   "metadata": {},
   "source": [
    "## Class Imbalance Analysis\n",
    "This cell checks whether the target classes are balanced. Imbalanced datasets may require resampling strategies (oversampling, undersampling, or class weights) during modeling to prevent the model from biasing toward the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Imbalance Check + Visuals\n",
    "\n",
    "# Compute counts & percentages\n",
    "counts = y_bin.value_counts().sort_index()\n",
    "pcts = (counts / len(y_bin) * 100).round(2)\n",
    "ratio = round(counts.max() / counts.min(), 2)\n",
    "\n",
    "print(\"Class Distribution Summary:\")\n",
    "print(f\"  Class 0: {counts[0]} samples ({pcts[0]}%)\")\n",
    "print(f\"  Class 1: {counts[1]} samples ({pcts[1]}%)\")\n",
    "print(f\"\\nImbalance Ratio: {ratio}:1\")\n",
    "\n",
    "# Recommendation\n",
    "if ratio > 1.5:\n",
    "    print(\"\\n Moderate class imbalance detected.\")\n",
    "    print(\"   • Consider class_weight='balanced'\")\n",
    "    print(\"   • SMOTE/oversampling are options\")\n",
    "    print(\"   • Use stratified CV\")\n",
    "    print(\"   • Track precision, recall, F1, ROC-AUC\")\n",
    "else:\n",
    "    print(\"\\n Classes are fairly balanced.\")\n",
    "\n",
    "# Visuals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "ax1.bar(['Class 0', 'Class 1'], counts.values, color=['#4D96FF', '#FF6B6B'])\n",
    "ax1.set_title(\"Class Count\")\n",
    "ax1.set_ylabel(\"Samples\")\n",
    "plt.savefig(\"class_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "# Pie chart\n",
    "ax2.pie(counts.values, labels=['Class 0', 'Class 1'], autopct='%1.1f%%',\n",
    "        colors=['#4D96FF', '#FF6B6B'], startangle=90)\n",
    "ax2.set_title(\"Class Percentage\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7acd3",
   "metadata": {},
   "source": [
    "### Scatter, Pairplot, and PCA Visuals\n",
    "\n",
    "This cell creates a few additional visualizations to explore how features relate\n",
    "to the target:\n",
    "\n",
    "- Scatter plot of the top two correlated features  \n",
    "- Pairplot for a small set of top features  \n",
    "- PCA 2D projection to visualize class separability  \n",
    "\n",
    "These help show patterns in the data before modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afbb92-7cf5-47ce-bb8a-487d85721057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter and PCA plots \n",
    "# 1) Scatter of two top correlated features\n",
    "# 2) Small pairplot for top features\n",
    "# 3) PCA 2D projection of the feature matrix\n",
    "\n",
    "# Ensure we have a binary target variable to color plots\n",
    "if 'y_bin' in globals():\n",
    "    ybin = y_bin\n",
    "else:\n",
    "    if y.dtype == 'O':\n",
    "        ybin = y.map(lambda v: 1 if str(v).lower().startswith('m') else 0)\n",
    "    else:\n",
    "        ybin = y\n",
    "\n",
    "# Compute simple absolute correlations with target and pick top features\n",
    "num_X = X.select_dtypes(include=[np.number])\n",
    "corr_with_target = num_X.corrwith(ybin).abs().sort_values(ascending=False)\n",
    "top_feats = corr_with_target.index.tolist()\n",
    "print('Top features (by abs corr):', top_feats[:6])\n",
    "\n",
    "# Scatter of two strongest features (if available)\n",
    "if len(top_feats) >= 2:\n",
    "    f1, f2 = top_feats[0], top_feats[1]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.scatterplot(data=df, x=f1, y=f2, hue=TARGET_COL, palette='Set1', alpha=0.7)\n",
    "    plt.title(f'{f1} vs {f2} — colored by {TARGET_COL}')\n",
    "    plt.xlabel(f1)\n",
    "    plt.ylabel(f2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numeric features for a two-feature scatter.')\n",
    "\n",
    "# Pairplot for a small subset (top 3 or 4 features)\n",
    "pair_feats = top_feats[:4] if len(top_feats) >= 2 else []\n",
    "if len(pair_feats) >= 2:\n",
    "    print('Drawing pairplot for:', pair_feats)\n",
    "    sns.pairplot(df[pair_feats + [TARGET_COL]], hue=TARGET_COL, diag_kind='kde', plot_kws={'alpha':0.6})\n",
    "    plt.suptitle('Pairplot — top correlated features', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# PCA 2D projection (use scaled features if available)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if 'X_scaled' in globals():\n",
    "    X_pca_in = X_scaled.copy()\n",
    "else:\n",
    "    # Start from numeric columns, coerce any non-numeric entries to NaN\n",
    "    X_pca_in = num_X.copy().apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X_pca_in = X_pca_in.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop columns that are entirely NaN (they provide no signal)\n",
    "all_nan_cols = X_pca_in.columns[X_pca_in.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    print(\"Dropping all-NaN columns before PCA:\", all_nan_cols)\n",
    "    X_pca_in = X_pca_in.drop(columns=all_nan_cols)\n",
    "\n",
    "# Impute remaining NaNs with column mean\n",
    "X_pca_in = X_pca_in.fillna(X_pca_in.mean())\n",
    "\n",
    "# Final sanity check\n",
    "assert not X_pca_in.isna().any().any(), \"X_pca_in still contains NaN values after imputation\"\n",
    "\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X2 = pca.fit_transform(X_pca_in)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x=X2[:,0], y=X2[:,1], hue=ybin, palette='Set1', alpha=0.8)\n",
    "plt.xlabel(f'PC1 ({explained[0]*100:.1f}% var)')\n",
    "plt.ylabel(f'PC2 ({explained[1]*100:.1f}% var)')\n",
    "plt.title('PCA 2D — features projected to 2D (colored by target)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc0040",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual EDA Summary — What We've Learned\n",
    "\n",
    "So far, we've done **manual, step-by-step EDA**:\n",
    "- Loaded and inspected the breast cancer dataset (569 samples, 30 features)\n",
    "- Checked for missing values, duplicates, and data types\n",
    "- Analyzed feature correlations and identified top features by absolute correlation with target\n",
    "- Ran t-tests to find statistically significant features (p < 0.05)\n",
    "- Detected outliers using the IQR method and visualized them\n",
    "- Confirmed class balance (relatively balanced, imbalance ratio ~1.05)\n",
    "- Generated KDE plots, scatter plots, and a PCA projection\n",
    "- Created ratio features (worst / mean) and dropped standard-error columns\n",
    "- Applied percentile capping (1st–99th) and StandardScaler normalization\n",
    "- Computed mutual information ranking of features\n",
    "\n",
    "**Key outputs saved**:\n",
    "- `data/engineered/breast_cancer_engineered.csv` — fully preprocessed dataset\n",
    "- `artifacts/engineering/transformers.pkl` — scaler and metadata\n",
    "- `artifacts/eda/mutual_info_ranking.csv` — feature importance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd54687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EDA Agent — this mirrors the manual steps above but in an automated, repeatable way\n",
    "class EDAAgent:\n",
    "    \"\"\"\n",
    "    EDA Agent: Automates the entire EDA pipeline.\n",
    "    \n",
    "    This agent performs the same steps as the manual EDA above, but:\n",
    "    - Logs each step with timestamps\n",
    "    - Returns structured results (logs + metrics)\n",
    "    - Can be reused and compared against the manual approach\n",
    "    \n",
    "    It's to compare:\n",
    "    1. Manual approach (step-by-step human workflow)\n",
    "    2. Agentic approach (automated pipeline with logs)\n",
    "    \n",
    "    Both should produce the same results, but the agent shows\n",
    "    how automation + logging enables reproducibility and auditability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42, verbose=True):\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.logs = []\n",
    "        self.results = {}\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Record a timestamped log entry.\"\"\"\n",
    "        ts = time.strftime('%H:%M:%S')\n",
    "        entry = f'[{ts}] {message}'\n",
    "        self.logs.append(entry)\n",
    "        if self.verbose:\n",
    "            print(entry)\n",
    "    \n",
    "    def load_and_inspect(self, df, target_col):\n",
    "        \"\"\"Step 1: Load dataset and perform basic checks.\"\"\"\n",
    "        self.log(f'Loading dataset: shape {df.shape}')\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isna().sum().sum()\n",
    "        if missing_count > 0:\n",
    "            self.log(f'⚠ Found {missing_count} missing values')\n",
    "        else:\n",
    "            self.log('✓ No missing values detected')\n",
    "        \n",
    "        # Check for duplicates\n",
    "        dup_count = df.duplicated().sum()\n",
    "        self.log(f'Duplicates: {dup_count} rows')\n",
    "        \n",
    "        # Target distribution\n",
    "        if target_col in df.columns:\n",
    "            counts = df[target_col].value_counts()\n",
    "            self.log(f'Target distribution: {dict(counts)}')\n",
    "        \n",
    "        self.results['dataset_shape'] = df.shape\n",
    "        return df\n",
    "    \n",
    "    def analyze_correlations(self, X, y):\n",
    "        \"\"\"Step 2: Compute and rank feature correlations with target.\"\"\"\n",
    "        self.log('Computing feature-target correlations...')\n",
    "        \n",
    "        # Handle binary target\n",
    "        if y.dtype == 'O' or y.dtype.name == 'category':\n",
    "            uniq = list(y.unique())\n",
    "            if len(uniq) == 2:\n",
    "                y_bin = y.map({uniq[0]: 0, uniq[1]: 1})\n",
    "            else:\n",
    "                y_bin = y\n",
    "        else:\n",
    "            y_bin = y\n",
    "        \n",
    "        # Compute absolute correlations\n",
    "        num_X = X.select_dtypes(include=[np.number])\n",
    "        corr_with_target = num_X.corrwith(y_bin).abs().sort_values(ascending=False)\n",
    "        \n",
    "        top_features = corr_with_target.head(10)\n",
    "        self.log(f'Top 10 correlated features: {list(top_features.index)}')\n",
    "        \n",
    "        self.results['top_correlated_features'] = top_features.to_dict()\n",
    "        return y_bin\n",
    "    \n",
    "    def run_statistical_tests(self, X, y_bin):\n",
    "        \"\"\"Step 3: Perform t-tests to identify statistically significant features.\"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        self.log('Running t-tests for statistical significance...')\n",
    "        \n",
    "        class_0 = y_bin.unique()[0]\n",
    "        class_1 = y_bin.unique()[1]\n",
    "        \n",
    "        X_class_0 = X[y_bin == class_0]\n",
    "        X_class_1 = X[y_bin == class_1]\n",
    "        \n",
    "        sig_count = 0\n",
    "        for col in X.select_dtypes(include=[np.number]).columns:\n",
    "            t_stat, p_val = stats.ttest_ind(X_class_0[col].dropna(), X_class_1[col].dropna())\n",
    "            if p_val < 0.05:\n",
    "                sig_count += 1\n",
    "        \n",
    "        self.log(f'Found {sig_count} statistically significant features (p < 0.05)')\n",
    "        self.results['significant_feature_count'] = sig_count\n",
    "        return sig_count\n",
    "    \n",
    "    def detect_outliers(self, X):\n",
    "        \"\"\"Step 4: Identify outliers using IQR method.\"\"\"\n",
    "        self.log('Detecting outliers using IQR method...')\n",
    "        \n",
    "        outlier_count = 0\n",
    "        for col in X.select_dtypes(include=[np.number]).columns:\n",
    "            Q1, Q3 = X[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            outliers = X[(X[col] < lower) | (X[col] > upper)]\n",
    "            outlier_count += len(outliers)\n",
    "        \n",
    "        self.log(f'Total outlier instances detected: {outlier_count}')\n",
    "        self.results['total_outliers'] = outlier_count\n",
    "    \n",
    "    def feature_engineering(self, X):\n",
    "        \"\"\"Step 5: Create ratio features and drop unnecessary columns.\"\"\"\n",
    "        self.log('Engineering features: creating ratios (worst / mean)...')\n",
    "        \n",
    "        X_fe = X.copy()\n",
    "        \n",
    "        # Create ratio features\n",
    "        pairs = [\n",
    "            ('radius_mean', 'radius_worst'),\n",
    "            ('texture_mean', 'texture_worst'),\n",
    "            ('perimeter_mean', 'perimeter_worst'),\n",
    "            ('area_mean', 'area_worst'),\n",
    "            ('smoothness_mean', 'smoothness_worst'),\n",
    "            ('compactness_mean', 'compactness_worst'),\n",
    "            ('concavity_mean', 'concavity_worst'),\n",
    "            ('concave points_mean', 'concave points_worst'),\n",
    "            ('symmetry_mean', 'symmetry_worst'),\n",
    "            ('fractal_dimension_mean', 'fractal_dimension_worst'),\n",
    "        ]\n",
    "        \n",
    "        ratio_count = 0\n",
    "        for a, b in pairs:\n",
    "            if a in X_fe.columns and b in X_fe.columns:\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    X_fe[f'{b}_over_{a}'] = X_fe[b] / X_fe[a]\n",
    "                ratio_count += 1\n",
    "        \n",
    "        self.log(f'Created {ratio_count} ratio features')\n",
    "        \n",
    "        # Drop standard-error columns\n",
    "        se_cols = [c for c in X_fe.columns if str(c).endswith('_se')]\n",
    "        X_fe = X_fe.drop(columns=se_cols, errors='ignore')\n",
    "        self.log(f'Dropped {len(se_cols)} standard-error columns')\n",
    "        \n",
    "        self.results['engineered_shape'] = X_fe.shape\n",
    "        return X_fe\n",
    "    \n",
    "    def scale_and_cap(self, X, lower_pct=0.01, upper_pct=0.99):\n",
    "        \"\"\"Step 6: Cap outliers at percentiles and apply StandardScaler.\"\"\"\n",
    "        self.log(f'Capping outliers at {lower_pct*100:.0f}th–{upper_pct*100:.0f}th percentiles...')\n",
    "        \n",
    "        X_cap = X.copy()\n",
    "        for col in X.select_dtypes(include=[np.number]).columns:\n",
    "            lo = X[col].quantile(lower_pct)\n",
    "            hi = X[col].quantile(upper_pct)\n",
    "            X_cap[col] = X_cap[col].clip(lo, hi)\n",
    "        \n",
    "        self.log('Applying StandardScaler to all numeric features...')\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X_cap), columns=X_cap.columns)\n",
    "        \n",
    "        self.log(f'Scaling complete. Final shape: {X_scaled.shape}')\n",
    "        self.results['scaled_shape'] = X_scaled.shape\n",
    "        \n",
    "        return X_scaled, scaler\n",
    "    \n",
    "    def compute_feature_importance(self, X, y):\n",
    "        \"\"\"Step 7: Compute mutual information feature ranking.\"\"\"\n",
    "        self.log('Computing mutual information feature ranking...')\n",
    "        \n",
    "        # Impute missing values for MI computation\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        X_imputed = pd.DataFrame(imp.fit_transform(X), columns=X.columns, index=X.index)\n",
    "        \n",
    "        # Handle binary target\n",
    "        if y.dtype == 'O' or y.dtype.name == 'category':\n",
    "            uniq = list(y.unique())\n",
    "            if len(uniq) == 2:\n",
    "                y_bin = y.map({uniq[0]: 0, uniq[1]: 1})\n",
    "            else:\n",
    "                y_bin = y\n",
    "        else:\n",
    "            y_bin = y\n",
    "        \n",
    "        # Compute MI\n",
    "        mi = mutual_info_classif(X_imputed, y_bin, random_state=self.random_state)\n",
    "        mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "        \n",
    "        top_mi = mi_series.head(5).to_dict()\n",
    "        self.log(f'Top 5 features by MI: {top_mi}')\n",
    "        \n",
    "        self.results['mi_ranking'] = mi_series.to_dict()\n",
    "        return mi_series\n",
    "    \n",
    "    def run(self, df, target_col, X_cols=None):\n",
    "        \"\"\"\n",
    "        Execute the full EDA pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: DataFrame with all features + target\n",
    "        - target_col: name of the target column\n",
    "        - X_cols: list of feature column names (if None, use all except target_col)\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary with logs and results\n",
    "        \"\"\"\n",
    "        self.log('='*60)\n",
    "        self.log('EDA AGENT STARTED')\n",
    "        self.log('='*60)\n",
    "        \n",
    "        # Step 1: Load and inspect\n",
    "        df = self.load_and_inspect(df, target_col)\n",
    "        \n",
    "        # Split features and target\n",
    "        y = df[target_col]\n",
    "        X = df.drop(columns=[target_col])\n",
    "        \n",
    "        # Step 2: Analyze correlations\n",
    "        y_bin = self.analyze_correlations(X, y)\n",
    "        \n",
    "        # Step 3: Statistical tests\n",
    "        self.run_statistical_tests(X, y_bin)\n",
    "        \n",
    "        # Step 4: Outlier detection\n",
    "        self.detect_outliers(X)\n",
    "        \n",
    "        # Step 5: Feature engineering\n",
    "        X_fe = self.feature_engineering(X)\n",
    "        \n",
    "        # Step 6: Scale and cap\n",
    "        X_scaled, scaler = self.scale_and_cap(X_fe)\n",
    "        \n",
    "        # Step 7: Feature importance\n",
    "        self.compute_feature_importance(X_scaled, y)\n",
    "        \n",
    "        self.log('='*60)\n",
    "        self.log('EDA AGENT FINISHED')\n",
    "        self.log('='*60)\n",
    "        \n",
    "        return {\n",
    "            'logs': self.logs,\n",
    "            'results': self.results,\n",
    "            'X_scaled': X_scaled,\n",
    "            'y': y,\n",
    "            'scaler': scaler\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb5559",
   "metadata": {},
   "source": [
    "---\n",
    "## Running the EDA Agent — Automated Pipeline with Logs\n",
    "\n",
    "Now let's instantiate the EDA Agent and run it on the same dataset.\n",
    "The agent will perform **identical steps** as our manual EDA, but with:\n",
    "- Timestamped logs for each operation\n",
    "- Structured results that can be inspected and compared\n",
    "- Reproducible, auditable output\n",
    "\n",
    "Watch the agent's logs below to see how it progresses through the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ae501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the EDA Agent and run it\n",
    "# Note: We're running it on the original df (same as our manual EDA above)\n",
    "\n",
    "agent = EDAAgent(random_state=RANDOM_STATE, verbose=True)\n",
    "agent_output = agent.run(df, target_col=TARGET_COL)\n",
    "\n",
    "print('\\n')\n",
    "print('Agent execution complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c0649",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual vs. Agentic EDA — Side-by-Side Comparison\n",
    "\n",
    "Below we compare the manual approach (human-driven, step-by-step) with the agentic approach (automated pipeline).\n",
    "\n",
    "**Key observations**:\n",
    "- Both approaches should produce **identical results** (same dataset, same transformations)\n",
    "- The agent provides **timestamped logs** showing exactly what happened and when\n",
    "- The agent's **structured output** makes it easy to extract metrics and verify reproducibility\n",
    "- For the professor: The logs show how automation eliminates manual errors and improves auditability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69712a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display agent logs for full transparency\n",
    "print('='*80)\n",
    "print('AGENT EXECUTION LOG (Timestamped Events)')\n",
    "print('='*80)\n",
    "for i, log_entry in enumerate(agent_output['logs'], 1):\n",
    "    print(f'{i:2d}. {log_entry}')\n",
    "\n",
    "print('\\n')\n",
    "print('='*80)\n",
    "print('AGENT RESULTS (Key Metrics Extracted)')\n",
    "print('='*80)\n",
    "\n",
    "results = agent_output['results']\n",
    "\n",
    "print(f\"\\nDataset Shape: {results['dataset_shape']}\")\n",
    "print(f\"Engineered Shape: {results['engineered_shape']}\")\n",
    "print(f\"Scaled Shape: {results['scaled_shape']}\")\n",
    "print(f\"Statistically Significant Features (p < 0.05): {results['significant_feature_count']}\")\n",
    "print(f\"Total Outlier Instances Detected: {results['total_outliers']}\")\n",
    "\n",
    "print('\\nTop Features by Correlation with Target:')\n",
    "for feat, corr in list(results['top_correlated_features'].items())[:5]:\n",
    "    print(f\"  {feat}: {corr:.4f}\")\n",
    "\n",
    "print('\\nTop Features by Mutual Information:')\n",
    "mi_dict = results['mi_ranking']\n",
    "for feat, mi_val in list(mi_dict.items())[:5]:\n",
    "    print(f\"  {feat}: {mi_val:.4f}\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON: Manual vs. Agentic Approach')\n",
    "print('='*80)\n",
    "print(f\"\"\"\n",
    "Manual EDA (Human-Driven):\n",
    "  Flexible, exploratory approach\n",
    "  Can inspect intermediate results\n",
    "  Error-prone if repeated\n",
    "  Hard to audit which steps were done\n",
    "  Difficult to scale to new datasets\n",
    "\n",
    "Agentic EDA (Automated Pipeline):\n",
    "  Reproducible — same steps every time\n",
    "  Auditable — full timestamped log of all operations\n",
    "  Easy to reuse on new datasets\n",
    "  Structured output for downstream analysis\n",
    "  Can be integrated into larger workflows\n",
    "  Less flexibility for ad-hoc exploration\n",
    "\n",
    "CONCLUSION:\n",
    "The agent produces the same EDA results as manual steps, but with reproducibility\n",
    "and auditability. Perfect for consistent data processing in production pipelines.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911fe9a",
   "metadata": {},
   "source": [
    "### Agentic-style Feature Engineering Demo\n",
    "\n",
    "This short, non-redundant demo shows a one-line 'agentic' FE step that reuses the existing helper functions. It demonstrates how an agent would apply the baseline FE reproducibly (no new FEAgent class required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a95326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic-style FE demo (reuses existing helper functions)\n",
    "X_fe_demo = add_ratio_features(X)\n",
    "X_fe_demo = drop_se_columns(X_fe_demo)\n",
    "X_fe_demo = cap_percentiles(X_fe_demo, lower=0.01, upper=0.99)\n",
    "print('Agentic-style FE demo shape:', X_fe_demo.shape)\n",
    "print('Sample engineered columns:', [c for c in X_fe_demo.columns if '_over_' in str(c)][:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5ccc1",
   "metadata": {},
   "source": [
    "## Baseline Feature Engineering\n",
    "This cell applies our baseline feature engineering: it creates ratio features (worst / mean) for key measurements and removes all _se standard-error columns to reduce noise and dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratio features and drop *_se columns\n",
    "def add_ratio_features(df):\n",
    "    df = df.copy()\n",
    "    pairs = [\n",
    "        ('radius_mean','radius_worst'),\n",
    "        ('texture_mean','texture_worst'),\n",
    "        ('perimeter_mean','perimeter_worst'),\n",
    "        ('area_mean','area_worst'),\n",
    "        ('smoothness_mean','smoothness_worst'),\n",
    "        ('compactness_mean','compactness_worst'),\n",
    "        ('concavity_mean','concavity_worst'),\n",
    "        ('concave points_mean','concave points_worst'),\n",
    "        ('symmetry_mean','symmetry_worst'),\n",
    "        ('fractal_dimension_mean','fractal_dimension_worst'),\n",
    "    ]\n",
    "    for a, b in pairs:\n",
    "        if a in df.columns and b in df.columns:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                df[f'{b}_over_{a}'] = df[b] / df[a]\n",
    "    return df\n",
    "\n",
    "def drop_se_columns(df):\n",
    "    return df.drop(columns=[c for c in df.columns if str(c).endswith('_se')], errors='ignore')\n",
    "\n",
    "X_fe = add_ratio_features(X)\n",
    "X_fe = drop_se_columns(X_fe)\n",
    "print(\"Feature-engineered shape:\", X_fe.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da325179",
   "metadata": {},
   "source": [
    "## Cap Percentiles, Scale, Save Engineered Data & Transformers\n",
    "This cell caps outliers using percentile clipping, scales all features with StandardScaler, and then saves the engineered dataset and transformer metadata for consistent reuse across the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef60e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_percentiles(df, lower=0.01, upper=0.99):\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        lo = df[col].quantile(lower)\n",
    "        hi = df[col].quantile(upper)\n",
    "        df[col] = df[col].clip(lo, hi)\n",
    "    return df\n",
    "\n",
    "lower_pct, upper_pct = 0.01, 0.99\n",
    "X_cap = cap_percentiles(X_fe, lower=lower_pct, upper=upper_pct)\n",
    "\n",
    "# Record clip bounds for metadata\n",
    "clip_lower = {}\n",
    "clip_upper = {}\n",
    "for col in X_fe.select_dtypes(include=[np.number]).columns:\n",
    "    clip_lower[col] = float(X_fe[col].quantile(lower_pct))\n",
    "    clip_upper[col] = float(X_fe[col].quantile(upper_pct))\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_cap), columns=X_cap.columns)\n",
    "\n",
    "fe_metadata = {\n",
    "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"clip_percentiles\": {\"lower\": lower_pct, \"upper\": upper_pct},\n",
    "    \"clip_bounds\": {\"lower\": clip_lower, \"upper\": clip_upper},\n",
    "    \"feature_columns\": list(X_scaled.columns),\n",
    "    \"transform\": \"cap_percentiles + StandardScaler\",\n",
    "}\n",
    "\n",
    "out_df = X_scaled.copy()\n",
    "out_df[TARGET_COL] = y.values\n",
    "\n",
    "ENGINEERED_PATH = DATA_ENGINEERED / \"breast_cancer_engineered.csv\"\n",
    "out_df.to_csv(ENGINEERED_PATH, index=False)\n",
    "\n",
    "joblib.dump({'scaler': scaler, 'columns': list(X_scaled.columns), 'fe_metadata': fe_metadata},\n",
    "            ARTIFACTS_ENG / 'transformers.pkl')\n",
    "\n",
    "print(\"Saved engineered data to:\", ENGINEERED_PATH)\n",
    "print(\"Saved transformers to:\", ARTIFACTS_ENG / \"transformers.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95757905",
   "metadata": {},
   "source": [
    "## Mutual Information Ranking\n",
    "This cell computes Mutual Information scores to measure how strongly each feature relates to the target, and saves a ranked list of the most informative features for later modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff516865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with column mean\n",
    "imp = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = pd.DataFrame(imp.fit_transform(X_scaled),\n",
    "                         columns=X_scaled.columns, index=X_scaled.index)\n",
    "\n",
    "# Select correct target (y_bin if present else y)\n",
    "target = y_bin if 'y_bin' in globals() else y\n",
    "\n",
    "# Compute mutual information\n",
    "mi = mutual_info_classif(X_imputed, target, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi, index=X_imputed.columns).sort_values(ascending=False)\n",
    "\n",
    "# Save ranking\n",
    "mi_csv_path = ARTIFACTS_EDA / \"mutual_info_ranking.csv\"\n",
    "mi_series.to_csv(mi_csv_path)\n",
    "print(\"Saved mutual info ranking to:\", mi_csv_path)\n",
    "\n",
    "mi_series.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f2b68",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "This cell splits the processed data into training and testing sets (using stratification) so we can train models fairly and evaluate them on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688018c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same target as MI (binary if available)\n",
    "target = y_bin if 'y_bin' in globals() else y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, target,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=target,\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99467ba",
   "metadata": {},
   "source": [
    "## XGBoost + GridSearchCV Modeling\n",
    "This cell trains an XGBoost model using GridSearchCV to find the best hyperparameters, then evaluates the final model on the test set using ROC-AUC, classification metrics, and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b49e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "# Hyperparameter grid (your earlier setup)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [400],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'reg_lambda': [0.1, 1, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "}\n",
    "\n",
    "# Stratified K-fold CV\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters Found:\", grid.best_params_)\n",
    "print(\"Best ROC-AUC Score from CV:\", grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_proba = grid.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_proba)\n",
    "print(\"Test ROC-AUC:\", test_auc)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test)\n",
    "plt.title(\"Confusion Matrix — Best XGBoost Model\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
