{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ee671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer EDA & Modeling Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52680588",
   "metadata": {},
   "source": [
    "## Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313fbf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import importlib.util\n",
    "import inspect\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Display + plotting style\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f87cbc",
   "metadata": {},
   "source": [
    "## Repository Root & Project Paths\n",
    "This cell detects the project’s root folder and sets up all directory paths (data, engineered data, artifacts) so the notebook can reliably read and save files no matter where it’s run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the project root so all file paths work no matter where the notebook is run\n",
    "def get_repo_root():\n",
    "    try:\n",
    "        # Ask Git for the top-level directory of this repository\n",
    "        return Path(subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], text=True).strip())\n",
    "    except Exception:\n",
    "        # Walk up a few levels looking for a .git folder\n",
    "        p = Path.cwd()\n",
    "        for _ in range(6):\n",
    "            if (p / \".git\").exists():\n",
    "                return p\n",
    "            p = p.parent\n",
    "        # Fallback: current working directory\n",
    "        return Path.cwd()\n",
    "\n",
    "try:\n",
    "    ROOT = get_repo_root()\n",
    "except Exception:\n",
    "    ROOT = Path.cwd()\n",
    "\n",
    "FALLBACK_ROOT = Path(r\"C:\\Users\\rajni\\Documents\\breast-cancer-agentic\")\n",
    "if ROOT != FALLBACK_ROOT and not (ROOT / \"data\" / \"raw\").exists():\n",
    "    ROOT = FALLBACK_ROOT\n",
    "\n",
    "print(\"Repo root:\", ROOT)\n",
    "\n",
    "# Define key directories\n",
    "DATA_RAW        = ROOT / \"data\" / \"raw\"\n",
    "DATA_ENGINEERED = ROOT / \"data\" / \"engineered\"\n",
    "ARTIFACTS_ENG   = ROOT / \"artifacts\" / \"engineering\"\n",
    "ARTIFACTS_EDA   = ROOT / \"artifacts\" / \"eda\"\n",
    "\n",
    "for p in [DATA_ENGINEERED, ARTIFACTS_ENG, ARTIFACTS_EDA]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Source CSV + target column\n",
    "SRC_FILE   = DATA_RAW / \"breast_cancer_with_columns.csv\"\n",
    "TARGET_COL = \"diagnosis\"\n",
    "\n",
    "print(\"Using source file:\", SRC_FILE)\n",
    "assert SRC_FILE.exists(), f\"Missing source file: {SRC_FILE}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0023fb",
   "metadata": {},
   "source": [
    "## Load Dataset & Basic Checks\n",
    "This cell loads the raw dataset, verifies that the target column exists, splits the data into features and labels, and provides basic previews (head, summary stats, missing values) to confirm the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from disk\n",
    "df = pd.read_csv(SRC_FILE)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n",
    "# Make sure the target column exists before continuing\n",
    "if TARGET_COL not in df.columns:\n",
    "    print(\"Columns available (first 40):\", list(df.columns)[:40])\n",
    "    raise AssertionError(f\"Target column '{TARGET_COL}' not found in CSV\")\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "print(\"Target distribution (normalized):\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Glance at the data and summary statistics to sanity-check values and types\n",
    "display(df.head())\n",
    "display(df.describe(include=\"all\").T)\n",
    "\n",
    "print(\"Missing values (top 15):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f6253",
   "metadata": {},
   "source": [
    "## Correlation Heatmap & Correlation with Target\n",
    "This cell computes a correlation matrix for all numeric features and visualizes it as a heatmap, then converts the diagnosis column to a binary target (y_bin) and ranks features by how strongly they’re linearly correlated with that target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap among numeric features\n",
    "corr = X.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Feature correlation heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target: handle object target by mapping to binary\n",
    "if y.dtype == 'O' or y.dtype.name == 'category':\n",
    "    uniq = list(y.unique())\n",
    "    if len(uniq) == 2:\n",
    "        y_bin = y.map({uniq[0]: 0, uniq[1]: 1})\n",
    "    else:\n",
    "        # fallback: try common names\n",
    "        y_bin = y.map(lambda v: 1 if str(v).lower().startswith('m') or str(v).lower().startswith('b') else 0)\n",
    "else:\n",
    "    y_bin = y\n",
    "\n",
    "tmp = X.copy()\n",
    "tmp[TARGET_COL] = y_bin.values\n",
    "target_corr = tmp.corr()[TARGET_COL].abs().sort_values(ascending=False)\n",
    "print(\"Top features by absolute correlation with target:\")\n",
    "display(target_corr.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335f718",
   "metadata": {},
   "source": [
    "## Numeric Overview & KDE Plots\n",
    "This cell reviews all numeric columns, checks missing/unique counts, and generates KDE plots to visualize how feature distributions differ between malignant and benign cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numeric overview and sample KDE plots\n",
    "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "print(\"Numeric columns count:\", len(num_cols))\n",
    "display(pd.DataFrame({\n",
    "    \"col\": num_cols,\n",
    "    \"n_missing\": [X[c].isna().sum() for c in num_cols],\n",
    "    \"n_unique\": [X[c].nunique() for c in num_cols],\n",
    "}).sort_values([\"n_missing\", \"n_unique\"], ascending=[False, True]).head(20))\n",
    "\n",
    "# Plot a few features\n",
    "sel = num_cols[:6]\n",
    "for col in sel:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.kdeplot(data=df, x=col, hue=TARGET_COL, fill=True, common_norm=False)\n",
    "    plt.title(f\"{col} by {TARGET_COL}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93238e",
   "metadata": {},
   "source": [
    "## Target Distribution Plot\n",
    "This cell creates a simple countplot of the diagnosis column to show the class distribution (malignant vs. benign) and verify the dataset’s balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83974cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=TARGET_COL, data=df,\n",
    "    palette=[\"#FF6B6B\", \"#4D96FF\"]   # red (malignant), blue (benign)\n",
    ")\n",
    "plt.title(\"Distribution of Diagnosis\")\n",
    "plt.xlabel(\"Diagnosis\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7acd3",
   "metadata": {},
   "source": [
    "## Agent Loader & Safe Adapter\n",
    "This cell makes sure our notebook can load and run the AI agents stored in the agents/ folder — no matter who runs the notebook or how their environment is set up.\n",
    "This cell handles these possible situations:\n",
    "1. Try normal package import\n",
    "2. If that fails, try loading agent modules directly from file paths\n",
    "3. If both imports fail, we use \"safe stub versions\"\n",
    "4. Safe-call adapter - No matter how the agent is written, the notebook can still call it without errors.\n",
    "5. Finally, it executes the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34148c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not set, adapter will only use in-memory df. DATA_PATH can be set to a CSV path\n",
    "# so agents that expect file-based input can be invoked without writing temp files.\n",
    "DATA_PATH = globals().get(\"DATA_PATH\", None)\n",
    "\n",
    "# Resolve project root so notebook works when executed from subfolders\n",
    "# If the notebook runs inside a `notebooks/` folder we assume the repo root is its parent.\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Directory where agent modules are expected to live (project-local adapters/agents)\n",
    "AGENTS_DIR = ROOT / \"agents\"\n",
    "\n",
    "# Ensure artifact directory exists and configure structured logging (file + console)\n",
    "os.makedirs(str(ARTIFACTS_EDA), exist_ok=True)\n",
    "log_file = ARTIFACTS_EDA / f\"agent_adapter_{datetime.now(datetime.UTC).strftime('%Y%m%dT%H%M%SZ')}.log\"\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                    filename=str(log_file))\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(logging.Formatter('%(asctime)s %(levelname)s: %(message)s'))\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "logging.info(\"ROOT: %s\", ROOT)\n",
    "logging.info(\"AGENTS_DIR: %s | exists: %s\", AGENTS_DIR, AGENTS_DIR.exists())\n",
    "logging.info(\"__init__.py present: %s\", (AGENTS_DIR / \"__init__.py\").exists())\n",
    "\n",
    "# Audit logger (writes newline-delimited JSON records to artifacts)\n",
    "try:\n",
    "    from agents.audit_logger import get_default_audit\n",
    "    audit = get_default_audit(artifacts_dir=ARTIFACTS_EDA)\n",
    "    logging.info(\"Audit logger initialized at: %s\", audit.filepath)\n",
    "except Exception as e_audit:\n",
    "    logging.info(\"Audit logger not available: %s\", e_audit)\n",
    "    audit = None\n",
    "\n",
    "# Privacy utilities (de-identification)\n",
    "try:\n",
    "    from agents.privacy import deidentify_to_temp_csv, deidentify_dataframe\n",
    "    logging.info(\"Privacy utilities available (deidentify_to_temp_csv)\")\n",
    "except Exception as e_priv:\n",
    "    logging.info(\"Privacy utilities not available: %s\", e_priv)\n",
    "    deidentify_to_temp_csv = None\n",
    "    deidentify_dataframe = None\n",
    "\n",
    "# Track whether we found real agent implementations\n",
    "AGENTS_AVAILABLE = False\n",
    "\n",
    "# Helper: import a module directly from a file path (keeps our flexible loading strategy)\n",
    "# This allows the notebook to load `agents/eda_agent.py` without requiring package install.\n",
    "def load_module_by_path(mod_name, path: Path):\n",
    "    spec = importlib.util.spec_from_file_location(mod_name, str(path))\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "# Import strategy (resilient):\n",
    "# 1) Try normal package import (preferred for packaged deployments)\n",
    "# 2) If that fails, try loading agent modules by explicit file path (developer convenience)\n",
    "# 3) If both fail, provide safe no-op stubs so the notebook remains runnable.\n",
    "try:\n",
    "    from agents.eda_agent import run_eda_report  # expected callable\n",
    "    from agents.fe_agent import propose_features, apply_features\n",
    "    AGENTS_AVAILABLE = True\n",
    "    logging.info(\"Imported agents via package imports.\")\n",
    "except Exception as e_pkg:\n",
    "    logging.info(\"Package import failed: %s\", e_pkg)\n",
    "    try:\n",
    "        # Try loading agent files directly from `agents/`\n",
    "        eda_path = AGENTS_DIR / \"eda_agent.py\"\n",
    "        fe_path = AGENTS_DIR / \"fe_agent.py\"\n",
    "\n",
    "        eda_mod = load_module_by_path(\"agents.eda_agent\", eda_path)\n",
    "        run_eda_report = getattr(eda_mod, \"run_eda_report\", None)\n",
    "\n",
    "        fe_mod = load_module_by_path(\"agents.fe_agent\", fe_path) if fe_path.exists() else None\n",
    "        propose_features = getattr(fe_mod, \"propose_features\", None) if fe_mod else None\n",
    "        apply_features = getattr(fe_mod, \"apply_features\", None) if fe_mod else None\n",
    "\n",
    "        if callable(run_eda_report):\n",
    "            AGENTS_AVAILABLE = True\n",
    "            logging.info(\"Loaded agents via file path.\")\n",
    "        else:\n",
    "            raise ImportError(\"run_eda_report not found in eda_agent.py\")\n",
    "    except Exception as e_path:\n",
    "        logging.info(\"File-path import failed: %s\", e_path)\n",
    "\n",
    "        # Safe no-op stubs: allow notebook to proceed even without real agents\n",
    "        def run_eda_report(df, target_col=None, out_dir=\"artifacts/eda\"):\n",
    "            # Stub: indicates no EDA was performed on purpose\n",
    "            logging.info(\"run_eda_report stub called — no real EDA performed.\")\n",
    "            return {\"summary_path\": None, \"out_dir\": out_dir}\n",
    "\n",
    "        def propose_features(X, y=None, max_interactions=20):\n",
    "            logging.info(\"propose_features stub called.\")\n",
    "            return []\n",
    "\n",
    "        def apply_features(X, proposals, dry_run=True, confirm=False):\n",
    "            \"\"\"Apply feature proposals to `X` (stubbed).\n",
    "\n",
    "            Behavior:\n",
    "            - `dry_run=True` (default): do not persist changes; return transformed DataFrame + metadata.\n",
    "            - `dry_run=False`: requires explicit confirmation. Confirmation sources (in order):\n",
    "                1) `confirm=True` argument\n",
    "                2) `globals().get('CONFIRM_APPLY', False)` set in the notebook\n",
    "                3) interactive user input (type 'yes')\n",
    "\n",
    "            This pattern prevents accidental destructive edits when running notebooks non-interactively.\n",
    "            \"\"\"\n",
    "            logging.info(\"apply_features called (dry_run=%s, confirm=%s)\", dry_run, confirm)\n",
    "            X_new = X.copy()\n",
    "            applied = []\n",
    "\n",
    "            # Emit an audit event for the apply attempt (if audit logger is present)\n",
    "            if 'audit' in globals() and audit is not None:\n",
    "                try:\n",
    "                    ctx = audit.audit_event(event_type='apply_features', actor='notebook', action='apply',\n",
    "                                            target=str(ARTIFACTS_ENG / 'transformers.pkl'),\n",
    "                                            details={'dry_run': dry_run, 'n_proposals': len(proposals) if proposals else 0})\n",
    "                except Exception:\n",
    "                    ctx = None\n",
    "            else:\n",
    "                ctx = None\n",
    "\n",
    "            if ctx:\n",
    "                cm = ctx.__enter__()\n",
    "\n",
    "            # In a real implementation we'd apply transformations described by `proposals`.\n",
    "            # Here the stub simply records proposal names to demonstrate metadata structure.\n",
    "            for p in proposals or []:\n",
    "                name = p.get(\"name\") if isinstance(p, dict) else str(p)\n",
    "                applied.append(name)\n",
    "\n",
    "            metadata = {\"applied\": applied, \"count\": len(applied)}\n",
    "\n",
    "            if dry_run:\n",
    "                logging.info(\"Dry-run mode: no changes persisted. Metadata: %s\", metadata)\n",
    "                if ctx:\n",
    "                    ctx.__exit__(None, None, None)\n",
    "                return X_new, metadata\n",
    "\n",
    "            # Non-dry run path: require confirmation\n",
    "            global_confirm = globals().get(\"CONFIRM_APPLY\", False)\n",
    "            if not confirm and not global_confirm:\n",
    "                # Try to prompt interactively; this will raise in headless CI if input() is unavailable\n",
    "                try:\n",
    "                    ans = input(\"Apply feature proposals to dataset? Type 'yes' to proceed: \").strip().lower()\n",
    "                    if ans != \"yes\":\n",
    "                        logging.info(\"User declined to apply features via prompt.\")\n",
    "                        raise RuntimeError(\"User declined to apply features (no changes made).\")\n",
    "                except Exception:\n",
    "                    raise RuntimeError(\"Confirmation required to apply features; set CONFIRM_APPLY=True or pass confirm=True.\")\n",
    "\n",
    "            # At this point we consider changes \"approved\". Stub does not persist to disk.\n",
    "            logging.info(\"Applying features (stub) — metadata: %s\", metadata)\n",
    "            if ctx:\n",
    "                ctx.__exit__(None, None, None)\n",
    "            return X_new, metadata\n",
    "\n",
    "        logging.info(\"Using stubbed agent functions (no-op).\")\n",
    "\n",
    "# Compatibility adapter: call run_eda_report regardless of its exact signature\n",
    "# The adapter supports functions that accept either a DataFrame (`df`) or a file path\n",
    "# argument (`dataset_path` or `path`), and it also tries common positional forms.\n",
    "def call_run_eda_safely(run_eda_report_fn, df, target_col, out_dir, data_path=None, **extra):\n",
    "    \"\"\"Call run_eda_report whether it expects a DataFrame or a file path.\n",
    "    The adapter inspects the function signature and tries the most likely calling conventions.\n",
    "    \"\"\"\n",
    "    if run_eda_report_fn is None or not callable(run_eda_report_fn):\n",
    "        raise RuntimeError(\"run_eda_report is not callable; agent import failed.\")\n",
    "\n",
    "    params = inspect.signature(run_eda_report_fn).parameters\n",
    "    kw = dict(target_col=target_col, out_dir=str(out_dir), **extra)\n",
    "\n",
    "    # Case 1: function expects an explicit DataFrame named `df`\n",
    "    if \"df\" in params:\n",
    "        return run_eda_report_fn(df=df, **kw)\n",
    "\n",
    "    # Case 2: function expects a dataset path argument (common in detached agents)\n",
    "    if \"dataset_path\" in params or \"path\" in params:\n",
    "        if data_path is None:\n",
    "            raise ValueError(\"Agent expects a file path — set DATA_PATH before calling.\")\n",
    "        key = \"dataset_path\" if \"dataset_path\" in params else \"path\"\n",
    "        kw[key] = data_path\n",
    "        return run_eda_report_fn(**kw)\n",
    "\n",
    "    # Case 3: positional arguments fallback (df, target_col, out_dir) or (path, target_col, out_dir)\n",
    "    try:\n",
    "        return run_eda_report_fn(df, target_col, str(out_dir), **extra)\n",
    "    except TypeError:\n",
    "        if data_path is None:\n",
    "            raise\n",
    "        return run_eda_report_fn(data_path, target_col, str(out_dir), **extra)\n",
    "\n",
    "# --- Safety improvement: run agents in a subprocess with timeout when possible ---\n",
    "# We prefer subprocess runs because they are interruptible via the OS, easier to sandbox,\n",
    "# and their stdout/stderr can be captured independently of the notebook process.\n",
    "import json, tempfile, textwrap\n",
    "\n",
    "# Write a DataFrame to a temp CSV and return the path. Caller should remove the file.\n",
    "def _df_to_temp_csv(df):\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "    df.to_csv(tmp.name, index=False)\n",
    "    tmp.close()\n",
    "    return tmp.name\n",
    "\n",
    "# Create a small runner script that imports the agent and calls it using a dataset_path.\n",
    "# The runner prints a JSON object to stdout so the parent process can parse structured results.\n",
    "def _write_runner_script(use_file_path=None):\n",
    "    if use_file_path:\n",
    "        # Load the agent directly from the source file to avoid altering sys.path in the\n",
    "        # runner process (safer and more deterministic).\n",
    "        import_block = f\"import importlib.util\\nspec = importlib.util.spec_from_file_location('agent_mod', r'{use_file_path}')\\nmod = importlib.util.module_from_spec(spec)\\nspec.loader.exec_module(mod)\\nfrom agent_mod import run_eda_report\"\n",
    "    else:\n",
    "        # Default to importing the package name `agents.eda_agent` in environments\n",
    "        # where the project is installed/packaged.\n",
    "        import_block = \"from agents.eda_agent import run_eda_report\"\n",
    "\n",
    "    script = textwrap.dedent(f\"\"\"\n",
    "import json, sys, traceback\n",
    "try:\n",
    "    {import_block}\n",
    "    dataset = sys.argv[1]\n",
    "    target = sys.argv[2]\n",
    "    outdir = sys.argv[3]\n",
    "    res = run_eda_report(dataset_path=dataset, target_col=target, out_dir=outdir)\n",
    "    print(json.dumps({{'success': True, 'result': res}}, default=str))\n",
    "except Exception as e:\n",
    "    print(json.dumps({{'success': False, 'error': str(e), 'traceback': traceback.format_exc()}}))\n",
    "\"\"\")\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".py\", mode=\"w\", encoding=\"utf-8\")\n",
    "    tmp.write(script)\n",
    "    tmp.close()\n",
    "    return tmp.name\n",
    "\n",
    "# Run the tiny runner script in a subprocess, capture stdout/stderr, save them to ARTIFACTS_EDA,\n",
    "# and return the parsed result object (or raise on errors).\n",
    "def run_eda_via_subprocess(eda_py_path_or_none, dataset_path, target_col, out_dir, timeout=30):\n",
    "    runner = _write_runner_script(use_file_path=eda_py_path_or_none)\n",
    "    try:\n",
    "        cmd = [sys.executable, runner, dataset_path, str(target_col), str(out_dir)]\n",
    "        cp = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)\n",
    "        stdout = cp.stdout.strip()\n",
    "        stderr = cp.stderr or ''\n",
    "\n",
    "        # Persist stdout/stderr to artifact logs for later debugging (timestamped)\n",
    "        ts = datetime.now(datetime.UTC).strftime('%Y%m%dT%H%M%SZ')\n",
    "        try:\n",
    "            out_path = ARTIFACTS_EDA / f'agent_stdout_{ts}.log'\n",
    "            err_path = ARTIFACTS_EDA / f'agent_stderr_{ts}.log'\n",
    "            out_path.write_text(cp.stdout)\n",
    "            err_path.write_text(cp.stderr or '')\n",
    "            logging.info(\"Saved agent stdout to %s and stderr to %s\", out_path, err_path)\n",
    "            # Record audit info about the subprocess result\n",
    "            try:\n",
    "                if 'audit' in globals() and audit is not None:\n",
    "                    audit.log('agent_subprocess', actor='agent_runner', action='subprocess_complete',\n",
    "                              target=str(dataset_path),\n",
    "                              details={'stdout_log': str(out_path), 'stderr_log': str(err_path), 'runner': str(runner)})\n",
    "            except Exception:\n",
    "                logging.exception(\"Failed to emit audit record for agent subprocess\")\n",
    "        except Exception as e_write:\n",
    "            logging.exception(\"Failed to write agent stdout/stderr to artifacts: %s\", e_write)\n",
    "\n",
    "        # Expect the runner to print a JSON object. If not, raise with output included.\n",
    "        try:\n",
    "            obj = json.loads(stdout)\n",
    "        except Exception:\n",
    "            logging.error(\"Agent runner did not return valid JSON. stdout=%r stderr=%r\", stdout, stderr)\n",
    "            raise RuntimeError(f\"Agent runner failed to return JSON. stdout={stdout!r}, stderr={cp.stderr!r}\")\n",
    "\n",
    "        if not obj.get('success'):\n",
    "            logging.error(\"Agent reported failure: %s\", obj.get('error',''))\n",
    "            raise RuntimeError('Agent error: ' + obj.get('error','') + '\\n' + obj.get('traceback',''))\n",
    "\n",
    "        logging.info(\"Agent subprocess completed successfully\")\n",
    "        return obj.get('result')\n",
    "    finally:\n",
    "        # Try to clean up the temporary runner script file.\n",
    "        try:\n",
    "            Path(runner).unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# High-level wrapper that prefers subprocess execution paths and falls back to in-process calls.\n",
    "def run_eda_with_timeout(run_eda_report_fn, df, target_col, out_dir, data_path=None, timeout=30):\n",
    "    \"\"\"High-level wrapper: prefer subprocess runs (file or package), otherwise in-process fallback.\n",
    "\n",
    "    Steps:\n",
    "    1. If `agents/eda_agent.py` exists locally, run it in a subprocess using that file.\n",
    "    2. Else try importing `agents.eda_agent` in a subprocess (works when project is installed).\n",
    "    3. If subprocess paths fail, fall back to calling the function in-process with a thread-based timeout.\n",
    "       Note: thread-based timeouts cannot terminate CPU-bound C extensions; subprocess is preferred.\n",
    "    \"\"\"\n",
    "    # 1) If there's a local eda_agent.py, prefer running it in a subprocess (safer)\n",
    "    eda_file = AGENTS_DIR / \"eda_agent.py\"\n",
    "    if eda_file.exists():\n",
    "        # prepare dataset path: de-identify if possible\n",
    "        if data_path:\n",
    "            ds = data_path\n",
    "        else:\n",
    "            deid_strategy = globals().get('DEID_STRATEGY', 'drop')\n",
    "            deid_cols = globals().get('DEID_COLS', None)\n",
    "            deid_salt = globals().get('DEID_SALT', 'agentic-default-salt')\n",
    "            if deidentify_to_temp_csv:\n",
    "                ds = deidentify_to_temp_csv(df, strategy=deid_strategy, cols=deid_cols, salt=deid_salt)\n",
    "                try:\n",
    "                    if audit:\n",
    "                        audit.log('deidentify', actor='notebook', action='deidentify_to_temp_csv', target=str(ds), details={'strategy': deid_strategy, 'cols': deid_cols})\n",
    "                except Exception:\n",
    "                    logging.exception(\"Failed to emit audit record for de-identification\")\n",
    "            else:\n",
    "                ds = _df_to_temp_csv(df)\n",
    "        try:\n",
    "            return run_eda_via_subprocess(str(eda_file), ds, target_col, out_dir, timeout=timeout)\n",
    "        finally:\n",
    "            if data_path is None:\n",
    "                try: Path(ds).unlink()\n",
    "                except: pass\n",
    "\n",
    "    # 2) Try package import path via subprocess\n",
    "    try:\n",
    "        __import__(\"agents.eda_agent\")\n",
    "        if data_path:\n",
    "            ds = data_path\n",
    "        else:\n",
    "            deid_strategy = globals().get('DEID_STRATEGY', 'drop')\n",
    "            deid_cols = globals().get('DEID_COLS', None)\n",
    "            deid_salt = globals().get('DEID_SALT', 'agentic-default-salt')\n",
    "            if deidentify_to_temp_csv:\n",
    "                ds = deidentify_to_temp_csv(df, strategy=deid_strategy, cols=deid_cols, salt=deid_salt)\n",
    "                try:\n",
    "                    if audit:\n",
    "                        audit.log('deidentify', actor='notebook', action='deidentify_to_temp_csv', target=str(ds), details={'strategy': deid_strategy, 'cols': deid_cols})\n",
    "                except Exception:\n",
    "                    logging.exception(\"Failed to emit audit record for de-identification\")\n",
    "            else:\n",
    "                ds = _df_to_temp_csv(df)\n",
    "        try:\n",
    "            return run_eda_via_subprocess(None, ds, target_col, out_dir, timeout=timeout)\n",
    "        finally:\n",
    "            if data_path is None:\n",
    "                try: Path(ds).unlink()\n",
    "                except: pass\n",
    "    except Exception as e_pkg2:\n",
    "        logging.info(\"Package-based subprocess run not available: %s\", e_pkg2)\n",
    "\n",
    "    # 3) Last resort: call in-process with a thread timeout (cannot kill CPU-bound tasks)\n",
    "    from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "    with ThreadPoolExecutor(max_workers=1) as ex:\n",
    "        fut = ex.submit(call_run_eda_safely, run_eda_report_fn, df, target_col, out_dir, data_path=data_path)\n",
    "        try:\n",
    "            return fut.result(timeout=timeout)\n",
    "        except TimeoutError:\n",
    "            raise TimeoutError(f\"Agent call timed out after {timeout} seconds (in-process fallback)\")\n",
    "\n",
    "# Execute via safe adapter (works for real agent or stub), with a sensible default timeout\n",
    "try:\n",
    "    logging.info(\"Calling run_eda_report via safe adapter (timeout=%ss)\", 30)\n",
    "    if \"df\" not in globals():\n",
    "        raise NameError(\"df is not defined in this notebook cell scope.\")\n",
    "    if audit:\n",
    "        with audit.audit_event(event_type='eda_run', actor='notebook', action='run_eda', target=str(SRC_FILE), details={'timeout': 30}):\n",
    "            result = run_eda_with_timeout(\n",
    "                run_eda_report_fn=run_eda_report,\n",
    "                df=df,\n",
    "                target_col=TARGET_COL,\n",
    "                out_dir=ARTIFACTS_EDA,\n",
    "                data_path=DATA_PATH,\n",
    "                timeout=30,  # seconds; adjust as needed\n",
    "            )\n",
    "    else:\n",
    "        result = run_eda_with_timeout(\n",
    "            run_eda_report_fn=run_eda_report,\n",
    "            df=df,\n",
    "            target_col=TARGET_COL,\n",
    "            out_dir=ARTIFACTS_EDA,\n",
    "            data_path=DATA_PATH,\n",
    "            timeout=30,  # seconds; adjust as needed\n",
    "        )\n",
    "    logging.info(\"run_eda_report executed; artifacts (if any) saved to: %s\", ARTIFACTS_EDA)\n",
    "    if result is not None:\n",
    "        try:\n",
    "            logging.info(\"Agent result keys: %s\", list(result.keys()))\n",
    "        except Exception:\n",
    "            logging.info(\"Agent result type: %s\", type(result))\n",
    "except Exception:\n",
    "    logging.exception(\"run_eda_report execution failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5ccc1",
   "metadata": {},
   "source": [
    "## Baseline Feature Engineering\n",
    "This cell applies our baseline feature engineering: it creates ratio features (worst / mean) for key measurements and removes all _se standard-error columns to reduce noise and dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratio features and drop *_se columns\n",
    "def add_ratio_features(df):\n",
    "    df = df.copy()\n",
    "    pairs = [\n",
    "        ('radius_mean','radius_worst'),\n",
    "        ('texture_mean','texture_worst'),\n",
    "        ('perimeter_mean','perimeter_worst'),\n",
    "        ('area_mean','area_worst'),\n",
    "        ('smoothness_mean','smoothness_worst'),\n",
    "        ('compactness_mean','compactness_worst'),\n",
    "        ('concavity_mean','concavity_worst'),\n",
    "        ('concave points_mean','concave points_worst'),\n",
    "        ('symmetry_mean','symmetry_worst'),\n",
    "        ('fractal_dimension_mean','fractal_dimension_worst'),\n",
    "    ]\n",
    "    for a, b in pairs:\n",
    "        if a in df.columns and b in df.columns:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                df[f'{b}_over_{a}'] = df[b] / df[a]\n",
    "    return df\n",
    "\n",
    "def drop_se_columns(df):\n",
    "    return df.drop(columns=[c for c in df.columns if str(c).endswith('_se')], errors='ignore')\n",
    "\n",
    "X_fe = add_ratio_features(X)\n",
    "X_fe = drop_se_columns(X_fe)\n",
    "print(\"Feature-engineered shape:\", X_fe.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da325179",
   "metadata": {},
   "source": [
    "## Cap Percentiles, Scale, Save Engineered Data & Transformers\n",
    "This cell caps outliers using percentile clipping, scales all features with StandardScaler, and then saves the engineered dataset and transformer metadata for consistent reuse across the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef60e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_percentiles(df, lower=0.01, upper=0.99):\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        lo = df[col].quantile(lower)\n",
    "        hi = df[col].quantile(upper)\n",
    "        df[col] = df[col].clip(lo, hi)\n",
    "    return df\n",
    "\n",
    "lower_pct, upper_pct = 0.01, 0.99\n",
    "X_cap = cap_percentiles(X_fe, lower=lower_pct, upper=upper_pct)\n",
    "\n",
    "# Record clip bounds for metadata\n",
    "clip_lower = {}\n",
    "clip_upper = {}\n",
    "for col in X_fe.select_dtypes(include=[np.number]).columns:\n",
    "    clip_lower[col] = float(X_fe[col].quantile(lower_pct))\n",
    "    clip_upper[col] = float(X_fe[col].quantile(upper_pct))\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_cap), columns=X_cap.columns)\n",
    "\n",
    "fe_metadata = {\n",
    "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"clip_percentiles\": {\"lower\": lower_pct, \"upper\": upper_pct},\n",
    "    \"clip_bounds\": {\"lower\": clip_lower, \"upper\": clip_upper},\n",
    "    \"feature_columns\": list(X_scaled.columns),\n",
    "    \"transform\": \"cap_percentiles + StandardScaler\",\n",
    "}\n",
    "\n",
    "out_df = X_scaled.copy()\n",
    "out_df[TARGET_COL] = y.values\n",
    "\n",
    "ENGINEERED_PATH = DATA_ENGINEERED / \"breast_cancer_engineered.csv\"\n",
    "out_df.to_csv(ENGINEERED_PATH, index=False)\n",
    "\n",
    "joblib.dump({'scaler': scaler, 'columns': list(X_scaled.columns), 'fe_metadata': fe_metadata},\n",
    "            ARTIFACTS_ENG / 'transformers.pkl')\n",
    "\n",
    "print(\"Saved engineered data to:\", ENGINEERED_PATH)\n",
    "print(\"Saved transformers to:\", ARTIFACTS_ENG / \"transformers.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95757905",
   "metadata": {},
   "source": [
    "## Mutual Information Ranking\n",
    "This cell computes Mutual Information scores to measure how strongly each feature relates to the target, and saves a ranked list of the most informative features for later modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff516865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with column mean\n",
    "imp = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = pd.DataFrame(imp.fit_transform(X_scaled),\n",
    "                         columns=X_scaled.columns, index=X_scaled.index)\n",
    "\n",
    "# Select correct target (y_bin if present else y)\n",
    "target = y_bin if 'y_bin' in globals() else y\n",
    "\n",
    "# Compute mutual information\n",
    "mi = mutual_info_classif(X_imputed, target, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi, index=X_imputed.columns).sort_values(ascending=False)\n",
    "\n",
    "# Save ranking\n",
    "mi_csv_path = ARTIFACTS_EDA / \"mutual_info_ranking.csv\"\n",
    "mi_series.to_csv(mi_csv_path)\n",
    "print(\"Saved mutual info ranking to:\", mi_csv_path)\n",
    "\n",
    "mi_series.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f2b68",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "This cell splits the processed data into training and testing sets (using stratification) so we can train models fairly and evaluate them on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688018c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same target as MI (binary if available)\n",
    "target = y_bin if 'y_bin' in globals() else y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, target,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=target,\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99467ba",
   "metadata": {},
   "source": [
    "## XGBoost + GridSearchCV Modeling\n",
    "This cell trains an XGBoost model using GridSearchCV to find the best hyperparameters, then evaluates the final model on the test set using ROC-AUC, classification metrics, and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b49e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "# Hyperparameter grid (your earlier setup)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [400],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'reg_lambda': [0.1, 1, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "}\n",
    "\n",
    "# Stratified K-fold CV\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters Found:\", grid.best_params_)\n",
    "print(\"Best ROC-AUC Score from CV:\", grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_proba = grid.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_proba)\n",
    "print(\"Test ROC-AUC:\", test_auc)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test)\n",
    "plt.title(\"Confusion Matrix — Best XGBoost Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afbb92-7cf5-47ce-bb8a-487d85721057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
